These are some of the projects Robert has done:

Tabular-Playground-Series---March-2022: Kaggle competition involving forecasting twelve-hours of traffic flow in a U.S. metropolis.
I started this kaggle competition by creating additional features such as “roadway” and “daytime_id”. These features are basically a way to take all of the direction and time features and condense them into two features. Next I constructed many different visuals using matplotlib and seaborn. These visuals can be found in the attached notebook named “Useful Visuals”. In this notebook I also used techniques such as mutual information regression, lasso regression, permutation importance, and SHAP to determine that the roadway feature was the most informative feature. Next I decided to plot histograms of the congestion values of all of the roadways. Since most of the histograms showed a relatively normal distribution I figured that a simple approach such as simply taking the median of each roadway might give a relatively high accuracy. Before constructing my model I decided to take a closer look at some of the roadways that did not look normally distributed, and I decided to plot histograms of each of these specific roadways for each daytime_id to see how the distribution changed throughout the day. What I was able to conclude was that each distribution of congestion values resembled the distribution of the congestion value of 20 minutes prior. Therefore, for my model, I decided I would group the data on the roadway and daytime_id features and take the median. I would also create a new lag feature that would contain the congestion value from the previous 20 minute interval. My final predictions would then be a linear combination of the median and the 20 minute lag value. The linear combination was constructed using an ensemble of graident boosted trees such as catBoost regressor, adaboost regressor, bagging regressor, and hist gradient boosting regressor. My model was built and trained in the notebook named “Very Simple Using the Median”. My final prediction gave a mean squared error score of 5.964. I came in 47th out of 956 which placed me in the top 5% for this competition.


Recipe Recommender Website Using Django: The goal of this project was to create a website that allows a user to search for recipes by ingredient, nutritional information, time to cook, dietary restriction, and ethnicity (Italian, Mexican, Japanese, etc). After searching for recipes a user can scroll through all the options that are presented and can like any desired recipes. After searching for recipes a user can then go to a personalized dashboard which presents all previously liked recipes plus recommended recipes which are based off of the recipes that the user liked.

The recipe data that was used was taken from the following Kaggle dataset: https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions The datasets that were used were PP_recipes.csv, PP_users.csv, and RAW_recipes.csv. These three datasets were combined to make the final dataset which consisted of the most rated recipes with an average rating of three or more. The final recipe data as well as all of the datasets used to make the final dataset are too large to keep in this repository, but the entire process of building the final dataset can be seen in the data_preprocessing.ipynb file.

The web framework used was python's Django. The final dataset made in the data_preprocessing.ipynb file was loaded into a Django model by uploading an excel extract. That entire process was done through the import_from_excel function in views.py which renders the templates named import_form.html and import_sucess.html. By uploading the recipe dataset into a model, the process of having to upload a data extract every time a view function is called was able to be avoided. The process of uploading a data extract took about 10-15 seconds and extracting data from a Django model is instantaneous so a significant amount of time was saved by uploading the data to a Django model. Django's built in authentication system was leveraged to allow users to make accounts and login in order to make a personalized user dashboard for each user.

The recommendation algorithm follows a Content-Based Filtering recommendation system in which the recipes that get recommended are the recipes that are most similar to recipes that the user has recently liked. The original data came with a tags attribute which consists of a list of tags that highlight keywords for each recipe. All unique tags for all recipes were made into one list and for each recipe a new feature called tag bins was created which consisted of 1 if the tag in the index of the complete tag list was in a specific recipe and 0 if not. This process can be found in the data_preprocessing.ipynb file. Therefore, to find the most similar recipe for a given recipe the cosine distance between the two tag bins are computed. This process can be found in the Similarity function of the recommendation.py file.


Sales-Forecasting: Kaggle competition in which synthetic data is used to forecast the sale of four different books across six different countries and two different stores.
My EDA notebook, labeled "Plotly Visuals - TPS Sep 2022", consists of various graphs that were made using the plotly package in python. I used plotly to create various histograms, line graphs, pie charts, as well as a Sankey Diagram. From these graphs I collected various insights into how the data behaves, most notably how the number of books sold is greatly influenced by the day of the week and month of the year. Also I discovered that one store that sold many more books than the other. Using these insights I decided to test a few tree models: random forrest, ada boost, catboost lgbm, and xgb boost. Randomized Search CV was used on each tree model to determine each model's best cross validation score, with SMAPE as the evaluation metric, and optimal parameters. I created a function to find the model with the best cv score and found it to be the LGBM Regressor model. After applying the model to the test data set I was able to get a SMAPE value of 5.85 allowing me to place in the top 25% out of 1381 competitors.


Tabular-Playground-Series---May-2022: Kaggle competition that involved conducting binary classification over 33 numerical features.
For this challenge I was given simulated manufacturing control data and was tasked to predict whether the machine is in state 0 or state 1. The data had various feature interactions that were important in determining the machine state. Some of the interactions that were found were that certain projections of the feature space were partitioned into three regions with differing target probabilities. The projections that were found to be useful were: the projection to f_02 and f_21, the projection to f_05 and f_22, and the projection to f_00+f_01 and f_26. For every projection a ternary categorical feature was created to determine which region a sample belongs. This resulted in the addition of the features i_02_21, i_05_22, and i_00_01_26. Next there was a categorial feature f_27, that was able to be deconstructed to provide much more information to my model. How this feature was constructed was by making each letter in the feature a separate column, and the specific location of the letter is noted and made into another feature. Any columns that contained all zeros were then removed. After this was complete I normalized my data and began constructing a sequential neural network model. Keras was used to make the neural network model, and my best model provided an area under the ROC curve of 0.997 which placed me in the top 25% (281/1115) for this competition.


Movie-Recommendation-Program-with-Django:A Django Program where a user enters his/her favorite movie and the release year and gets recommended another movie
For this project I made a movie recommendation app that takes a movie title and a release year as input and outputs a similar movie. The algorithm for the recommendation process follows a rule based approach. I tried various other methods such as kmeans clustering and found that given the data I had access to a rules based method was the best to go. I got the original dataset from Kaggle.com and it consists of around 300,000 different movies from various years and various countries. The dataset also includes features such as the popularity of each movie, the IMDB average ratings and number of ratings, the release date, the original language, the genre, and an overview of the movie. I decided to recommend movies based on genre, language, release year, and popularity. Since many movies have many different genres I decided to limit the number of genres for each movie to two in order to prevent very popular movies with many genres being recommended the majority of the time. I then converted the movie dataset into a dictionary and pickled it so that it may be quickly loaded and the Django program may run faster. Once a user inputs a movie title and release date the program finds the movie and its index within the movie data dictionary. If the movie title cannot be found or if the release date is incorrect the program will output “Movie Not Found. Make Sure Spelling And Release Date Are Correct”. The release date of the movie is needed because there are many movies with the same title, the release date helps ensure that the correct movie is found within the movie dictionary. Once the movie index is found the genres and original language of the movie are also found. Next the program begins the rules based approach of recommending a movie. All movies with the same genre as the input movie are found and stored in a list. Then the movies are filtered based on the release year in which movies with a similar release year are added to a second list. Next the movies are filtered on language in which only the movies with the same original language are added to a third list. Lastly the remaining movies are added to a dictionary and sorted by popularity. The movie with the highest popularity, given it is not the input movie, is returned to the user along with an overview of the movie.



Income-Prediction-App-With-Django:For this project I made an income prediction app that predicts if a person has an income greater than or less than 50,000. The features that are used to make this prediction are a person’s age, how many years of education they completed, how many hours per week they work, what sector they work in, their marital status, their family relationships, their race, and their sex. The model that is used to make this classification is a simple logistic regression model. The attached notebook file, income_model.ipynb, shows the model training process. A logistic regression model was chosen due to its better performance compared to the other models. The data that was used to train the model was found on Kaggle.com and the dataset can be found in the files for this repository. Django was used to create the graphical user interface. The website that was created allows a user to input the necessary features and click the submit button. Once the submit button is pressed the corresponding classification is shown. The features that the user input as well as the classification the model gave are then stored in a database that can be viewed by the user by either clicking on DB at the top of the page or by clicking on View DB after clicking submit.



Movie-Sentiment-Program: This is the code for a program that uses Natural Language Processing to determine if a movie review is positive or negative. There were three steps to making this program. The first step was to find a dataset that the neural network model could be trained on. The dataset I chose was an IMBD dataset in a csv file that I found on Kaggle.com. The dataset contained 40,000 long movie reviews (most were over 200 words) and the corresponding sentiment labeled as either 1 (positive review) or 0 (negative review). After I found good training data I downloaded the csv file to a jupyter notebook so I could prepare the data for the neural network model. The attached jupyter notebook labeled data_cleaning shows the data cleaning process. In this notebook I cleaned the movie reviews by creating functions that would remove html, emojis, punctuation, numbers, stopwords, and also performing lemmatization on all of the text. After this I saved the cleaned text to a new csv file named movie_reviews.csv and moved to another jupyter notebook to begin the second step which was training my neural network model. The attached notebook file called setiment_model is where I trained my neural network model. Before I could feed any data into the model the texts first had to be converted to numbers. I first converted all the movie reviews into a list and then I defined the variables vocab, oov, embedding, padding, truncate, and max length. These variables are needed for the process of converting text into numbers that can be fed into a neural network model. After separating training and validation data I then finally tokenized the text (converted each word to a number). Finally a neural network model was trained on the tokenized text data. The neural network had five layers including an embedding layer and a pooling layer. 25 epochs were used to train the model and the validation data showed the model had an accuracy of 0.878. After the model was trained I saved the model and the tokenizer so that they could be used in the last step. The last step was creating the graphical user interface. The GUI was made with tkinter and can be found in the attached python file named movie_sentiment.py. After creating the basic architecture of the GUI I was able to load the model and tokenizer from the previous notebook and be able to take in a user input, in which a three or more sentence movie review is written, and determine whether this was a positive movie review or a negative movie review.



Tabular-Playground---January-2022: Kaggle competition in which a dataset containing five features must be used to predict the number of products sold For this challenge I predicted a full year worth of sales for three items at two stores located in three different countries. This dataset is completely fictional, but contains many effects that can be seen in real-world data, e.g., weekend and holiday effect, seasonality, etc. The first thing I did was make the date feature more usable. I did this by using python's datetime package to determine which day of the week each date was. I also used the holidays package to determine if a specific day was a holiday. Next I split the day, month, and year parts of the date feature so that they would become three separate columns. Lastly I found a dataset that contained the GDP per capita of each of the three countries that the stores are located in and I added that feature to the testing and training datasets. Once the preprocessing of the data was complete I began testing some models. The scoring criteria was the symmetric mean absolute percentage error, so I made a function called smape that could be used with python’s RandomizedSearchCV in order to find the best model according to the smape scoring metric. RandomizedSearchCV was used to test various parameters for a random forest model, a regression model, an ada boost model, a cat boost model, an xgboost model, and an lgbm regressor model. After testing various parameters on all these different models the random forest model was the model that ended up giving me my best smape score of 8.33. I also tried to use a neural network model using keras, however the random forest model still produced better results.



















